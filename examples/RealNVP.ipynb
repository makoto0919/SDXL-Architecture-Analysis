{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizing Flow Example: Real NVP\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EmoryMLIP/DeepGenerativeModelingIntro/blob/main/examples/RealNVP.ipynb)\n",
        "\n",
        "## Some References\n",
        "\n",
        "- Original work by [Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy (2016)](https://arxiv.org/abs/1605.08803)\n",
        "- Similar tutorial by [Arsenii Senya Ashukha](https://github.com/senya-ashukha/real-nvp-pytorch)\n",
        "- Section 3.1 of our paper  [Introduction to Deep Generative Modeling](https://arxiv.org/abs/2103.05180)\n",
        "\n",
        "## Short Mathematical Description\n",
        "\n",
        "We train the generator by minimizing the negative log-likelihood of the samples:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\newcommand{\\bftheta}{\\boldsymbol{\\theta}}\n",
        "    \\newcommand{\\bfx}{\\mathbf{x}}\n",
        "    \\newcommand{\\bfz}{\\mathbf{z}}\n",
        "    \\newcommand{\\bfy}{\\mathbf{y}}\n",
        "\\min_{\\bftheta}  \\frac{1}{s} \\sum_{i=1}^s \\left( {\\frac{1}{2}}\\left\\| g_{\\bftheta}^{-1}\\left(\\bfx^{(i)}\\right)\\right\\|^2 - \\log\\det\\nabla  g_{\\bftheta}^{-1}\\left(\\bfx^{(i)}\\right) \\right),\n",
        "\\end{equation}\n",
        "where $\\bfx^{(1)}, \\ldots$ are sampled i.i.d. from the data distribution. Here, the idea is to use a generator like\n",
        "\\begin{equation*}\n",
        "    g_{\\bftheta}(\\bfz) = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(\\bfz)\n",
        "\\end{equation*}\n",
        "to obtain a tractable inverse and log-determinant."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "DVeU2kZd8lZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepGenerativeModelingIntro'...\n"
          ]
        }
      ],
      "source": [
        "# install requirements  (Colab only)\n",
        "import sys,os\n",
        "if 'google.colab' in sys.modules:\n",
        "    dgm_dir = '/content/DeepGenerativeModelingIntro/'\n",
        "    if not os.path.exists(dgm_dir):\n",
        "        !git clone git://github.com/EmoryMLIP/DeepGenerativeModelingIntro.git\n",
        "\n",
        "    sys.path.append(os.path.dirname(dgm_dir))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uDmAcn598lZh",
        "outputId": "09d6d5b2-5343-4996-f1f4-54e87484613b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dg0MA2UK8lZj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch import distributions\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "plt.rcParams.update({'image.interpolation' : None})\n",
        "plt.rcParams['figure.figsize'] = [10, 7]\n",
        "plt.rcParams['figure.dpi'] = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some suggested experiments\n",
        "\n",
        "1. reduce the widths of the moon shapes so that the intrinsic dimensionality reduces to one\n",
        "1. try the other dataset or create your own more challenging distribution\n",
        "1. vary the hyperparameters (learning rate, number of layers, width, ...) and see how the performance changes\n",
        "1. (for later as this takes more time) use a different layer with tractable inverse and log-determinant\n",
        "\n",
        "## Setup the Dataset and Plotting"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "__wt50Fk8lZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "dataset = 'moons'\n",
        "# dataset = 'uniform'\n",
        "\n",
        "if dataset == 'moons':\n",
        "    # grid of the spatial domain (for plotting)\n",
        "    domain = (-1.2, 2.1, -1.2, 2.1)\n",
        "\n",
        "    def get_data(batch_size,noise=0.05):\n",
        "        return torch.tensor(datasets.make_moons(n_samples=batch_size, noise=noise)[0], dtype=torch.float32)\n",
        "elif dataset == 'uniform':\n",
        "    domain = (-1.0, 3.0, -1.0, 3.0)\n",
        "    def get_data(batch_size):\n",
        "        return torch.rand((batch_size,2)) + 1.0\n",
        "else:\n",
        "    print(\"incorrect dataset\")\n",
        "\n",
        "# grid of the spatial domain (for plotting)\n",
        "x1 = torch.linspace(domain[0],domain[1], 100)\n",
        "x2 = torch.linspace(domain[2],domain[3], 100)\n",
        "xg = torch.meshgrid(x1, x2)\n",
        "xx = torch.cat((xg[0].reshape(-1, 1), xg[1].reshape(-1, 1)), 1)\n",
        "\n",
        "from plotting import *\n",
        "\n",
        "xs = get_data(1000)\n",
        "plot_x(xs,domain)\n",
        "plt.title(\"dataset: %s\" % (dataset))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4SuLsTor8lZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose the Network Architecture\n",
        "\n",
        "We write the generator as\n",
        "\n",
        "\\begin{equation*}\n",
        "    \\newcommand{\\bftheta}{\\boldsymbol{\\theta}}\n",
        "\t\\newcommand{\\bfz}{\\mathbf{z}}\n",
        "\t\\newcommand{\\bfy}{\\mathbf{y}}\n",
        "\t\\newcommand{\\bfm}{\\mathbf{m}}\n",
        "\tg_{\\bftheta}(\\bfz) = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(\\bfz)\n",
        "\\end{equation*}\n",
        "\n",
        "Here we are using the Real NVP layer\n",
        "\n",
        "\\begin{equation*}\n",
        "f_j\\left(\\bfy^{(j)}\\right) = \\bfm^{(j)} \\odot \\bfy^{(j)} + (1-\\bfm^{(j)}) \\odot \\left(y^{(j)} \\odot \\exp\\left(s( \\bfy^{(j)} \\odot \\bfm^{(j)})\\right) + t\\left(\\bfy^{(j)} \\odot \\bfm^{(j)}\\right)\\right),\n",
        "\\end{equation*}\n",
        "\n",
        "where $\\bfm^{j} \\in \\{0,1\\}^n$ is used to mask some components of the inputs. Below, we alternate between the first and second coordinate."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gSYKpmDk8lZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "width = 128   # width of neural net layers\n",
        "K = 6         # number of layers\n",
        "\n",
        "from realNVP import NF, RealNVPLayer\n",
        "\n",
        "layers = torch.nn.ModuleList()\n",
        "for k in range(K):\n",
        "    m = torch.tensor([1 - (k % 2), k % 2])\n",
        "    t = nn.Sequential(nn.Linear(2, width), nn.LeakyReLU(), nn.Linear(width, width), nn.LeakyReLU(), nn.Linear(width, 2),\n",
        "                      nn.Tanh())\n",
        "    s = nn.Sequential(nn.Linear(2, width), nn.LeakyReLU(), nn.Linear(width, width), nn.LeakyReLU(), nn.Linear(width, 2),\n",
        "                      nn.Tanh())\n",
        "    layer = RealNVPLayer(s, t, m)\n",
        "    layers.append(layer)\n",
        "\n",
        "prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
        "flow = NF(layers, prior)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NQVo7w_n8lZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Generator\n",
        "\n",
        "Here, we use ADAM, a stochastic approximation scheme that operates on minibatches."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Qxs_Hr1P8lZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "num_steps = 10000      # number of training steps\n",
        "out_file=None          # base filename saving trained model\n",
        "plot_interval = 1000    # interval for visualizing intermediate results\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(flow.parameters(), lr=1e-4)\n",
        "his = np.zeros((0,1))\n",
        "\n",
        "print((3*\"--\" + \" K=%d, width=%d, batch_size=%d, num_steps=%d\" + 3*\"--\") % (K, width, batch_size, num_steps, ))\n",
        "\n",
        "if out_file is not None:\n",
        "    import os\n",
        "    out_dir, fname = os.path.split(out_file)\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "    print((3*\"--\" + \"out_file: %s\" + 3*\"--\") % (out_file))\n",
        "\n",
        "print((2*\"%7s    \") % (\"step\",\"J_ML\"))\n",
        "\n",
        "train_JML = 0.0\n",
        "num_step = 0\n",
        "\n",
        "for step in range(num_steps):\n",
        "\n",
        "    x = get_data(batch_size)\n",
        "    loss = -flow.log_prob(x).mean()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_JML += loss.item()\n",
        "    num_step += 1\n",
        "\n",
        "    if (step + 1) % plot_interval == 0:\n",
        "        train_JML /= num_step\n",
        "\n",
        "        print((\"%06d   \" + \"%1.4e  \") %\n",
        "              (step + 1, train_JML))\n",
        "        his = np.vstack([his, [train_JML]])\n",
        "        train_JML = 0.0\n",
        "        num_step = 0\n",
        "\n",
        "\n",
        "        zs = flow.ginv(x)[0].detach()\n",
        "        xs = flow.sample(200).detach()\n",
        "        log_px = flow.log_prob(xx).detach()\n",
        "\n",
        "        plt.Figure()\n",
        "        plt.subplot(1,3,1)\n",
        "        plot_x(xs,domain)\n",
        "        plt.title(\"generated samples\")\n",
        "\n",
        "        plt.subplot(1,3,2)\n",
        "        plot_z(zs)\n",
        "        plt.title(\"latent space\")\n",
        "\n",
        "        plt.subplot(1,3,3)\n",
        "        plot_px(log_px.reshape(len(x1), len(x2)),domain)\n",
        "        plt.title(\"likelihood estimate\")\n",
        "        plt.margins(0, 0)\n",
        "        if out_file is not None:\n",
        "            plt.savefig(\"%s-step-%d.png\" % (out_file,step+1), bbox_inches='tight', pad_inches=0)\n",
        "        plt.show()\n",
        "\n",
        "if out_file is not None:\n",
        "    torch.save(flow.state_dict(), (\"%s.pt\") % (out_file))\n",
        "    from scipy.io import savemat\n",
        "    savemat((\"%s.mat\") % (out_file), {\"his\":his})"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_crEbrQf8lZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Final Result\n",
        "\n",
        "Compare this to Figure 5 in [https://arxiv.org/abs/2103.05180](https://arxiv.org/abs/2103.05180)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "pRrRRH4F8lZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.Figure()\n",
        "plt.subplot(1,3,1)\n",
        "plot_px(log_px.reshape(len(x1), len(x2)),domain)\n",
        "plot_x(xs[:64, :],domain)\n",
        "plt.title(\"likelihood and samples\")\n",
        "\n",
        "xx = get_data(20000)\n",
        "zz = flow.ginv(xx)[0].detach().numpy()\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plot_pz(zz)\n",
        "plot_z(zs[:64,:])\n",
        "plt.title(\"inverse of generator\")\n",
        "\n",
        "zall = torch.cat(flow.ginv(x)[2],1)\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(x[:, 0], x[:, 1], \"bs\",alpha=0.05)\n",
        "plt.plot(zs[:, 0], zs[:, 1], \"or\",alpha=0.05)\n",
        "\n",
        "for k in range(6):\n",
        "    zk = zall[k,:].reshape(-1,2)\n",
        "    plt.plot(zk[:, 0], zk[:, 1], \"-xk\")\n",
        "    plt.plot(zk[0, 0], zk[0, 1], \"bs\")\n",
        "    plt.plot(zk[-1, 0], zk[-1, 1], \"or\")\n",
        "\n",
        "plt.axis(\"square\")\n",
        "plt.axis((-3.5, 3.5, -3.5, 3.5))\n",
        "plt.xticks((-3.5, 3.5))\n",
        "plt.yticks((-3.5, 3.5))\n",
        "plt.margins(0, 0)\n",
        "plt.title(\"hidden layers\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NqVqveBe8lZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fnMfqCDW8lZl"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}